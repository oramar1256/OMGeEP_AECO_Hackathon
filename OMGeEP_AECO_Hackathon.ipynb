{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4e7383a-d9b6-4083-aa7f-83b020bce89c",
   "metadata": {},
   "source": [
    "# OMGeEP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6389f2fc-0d41-40e1-b312-e31a75477049",
   "metadata": {},
   "source": [
    "# Download dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3e596609-0e5c-4259-8842-325ff70bbb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /opt/conda/lib/python3.13/site-packages (4.6.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.13/site-packages (1.7.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.13/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.13/site-packages (2.2.6)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.13/site-packages (1.5.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.13/site-packages (from lightgbm) (1.16.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting iterative-stratification\n",
      "  Downloading iterative_stratification-0.1.9-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.13/site-packages (from iterative-stratification) (2.2.6)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.13/site-packages (from iterative-stratification) (1.16.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.13/site-packages (from iterative-stratification) (1.7.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.13/site-packages (from scikit-learn->iterative-stratification) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.13/site-packages (from scikit-learn->iterative-stratification) (3.6.0)\n",
      "Downloading iterative_stratification-0.1.9-py3-none-any.whl (8.5 kB)\n",
      "Installing collected packages: iterative-stratification\n",
      "Successfully installed iterative-stratification-0.1.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting shap\n",
      "  Downloading shap-0.48.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.13/site-packages (from shap) (2.2.6)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.13/site-packages (from shap) (1.16.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.13/site-packages (from shap) (1.7.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.13/site-packages (from shap) (2.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /opt/conda/lib/python3.13/site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in /opt/conda/lib/python3.13/site-packages (from shap) (25.0)\n",
      "Collecting slicer==0.0.8 (from shap)\n",
      "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: numba>=0.54 in /opt/conda/lib/python3.13/site-packages (from shap) (0.61.2)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.13/site-packages (from shap) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.13/site-packages (from shap) (4.15.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /opt/conda/lib/python3.13/site-packages (from numba>=0.54->shap) (0.44.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.13/site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.13/site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.13/site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.13/site-packages (from scikit-learn->shap) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.13/site-packages (from scikit-learn->shap) (3.6.0)\n",
      "Downloading shap-0.48.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: slicer, shap\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [shap][32m1/2\u001b[0m [shap]\n",
      "\u001b[1A\u001b[2KSuccessfully installed shap-0.48.0 slicer-0.0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lightgbm scikit-learn pandas numpy joblib\n",
    "%pip install iterative-stratification\n",
    "%pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c34a2a-3329-4861-b036-7f752d62577c",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fc9fce67-4bca-4453-a71b-b5dc12ca2d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score, hamming_loss, roc_auc_score\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Optional: iterative stratification\n",
    "try:\n",
    "    from iterstrat.ml_stratifiers import iterative_train_test_split\n",
    "    ITERATIVE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    ITERATIVE_AVAILABLE = False\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352e2e19-5434-471d-8ffe-b86b48f18620",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7a8c096e-21cd-4b3a-a9f0-8a5c832fd418",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'ifbdata/atlanteco_hack/OMGeEP/output_files'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddf25c6-2772-48a6-b4bf-aaee40e147f8",
   "metadata": {},
   "source": [
    "# Read genomic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8682ef88-3fb2-425c-b5c1-8dfc15d597eb",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2a625008-b20f-4735-871d-840673f78eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_gene_abundances(df:pd.DataFrame, method:str='tss', id_col:str=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize gene abundance data for comparison between samples.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with gene IDs in first column and samples as remaining columns\n",
    "    method : str, default 'tss'\n",
    "        Normalization method:\n",
    "        - 'tss': Total Sum Scaling (relative abundance, sums to 1)\n",
    "        - 'tss_percent': Total Sum Scaling as percentages (sums to 100)\n",
    "        - 'z_score': Z-score normalization (mean=0, std=1)\n",
    "        - 'min_max': Min-max scaling (0 to 1 range)\n",
    "        - 'log_tss': Log-transformed TSS (log10(TSS + pseudocount))\n",
    "        - 'clr': Centered Log Ratio transformation\n",
    "    id_col : str, optional\n",
    "        Name of ID column. If None, assumes first column is ID\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Normalized DataFrame with same structure as input\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a copy to avoid modifying original\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    # Identify ID column\n",
    "    if id_col is None:\n",
    "        id_col = df.columns[0]\n",
    "    \n",
    "    # Get sample columns (all except ID column)\n",
    "    sample_cols = [col for col in df.columns if col != id_col]\n",
    "    \n",
    "    # Extract abundance matrix\n",
    "    abundance_matrix = df_norm[sample_cols].values\n",
    "    \n",
    "    if method == 'tss':\n",
    "        # Total Sum Scaling - convert to relative abundances\n",
    "        col_sums = abundance_matrix.sum(axis=0)\n",
    "        normalized_matrix = abundance_matrix / col_sums\n",
    "        \n",
    "    elif method == 'tss_percent':\n",
    "        # Total Sum Scaling as percentages\n",
    "        col_sums = abundance_matrix.sum(axis=0)\n",
    "        normalized_matrix = (abundance_matrix / col_sums) * 100\n",
    "        \n",
    "    elif method == 'z_score':\n",
    "        # Z-score normalization (standardization)\n",
    "        normalized_matrix = (abundance_matrix - abundance_matrix.mean(axis=0)) / abundance_matrix.std(axis=0)\n",
    "        \n",
    "    elif method == 'min_max':\n",
    "        # Min-max scaling to [0, 1] range\n",
    "        min_vals = abundance_matrix.min(axis=0)\n",
    "        max_vals = abundance_matrix.max(axis=0)\n",
    "        normalized_matrix = (abundance_matrix - min_vals) / (max_vals - min_vals)\n",
    "        \n",
    "    elif method == 'log_tss':\n",
    "        # Log-transformed TSS (common in metagenomics)\n",
    "        col_sums = abundance_matrix.sum(axis=0)\n",
    "        tss_matrix = abundance_matrix / col_sums\n",
    "        # Add small pseudocount to avoid log(0)\n",
    "        pseudocount = 1e-10\n",
    "        normalized_matrix = np.log10(tss_matrix + pseudocount)\n",
    "        \n",
    "    elif method == 'clr':\n",
    "        # Centered Log Ratio transformation\n",
    "        # Add small pseudocount to avoid log(0)\n",
    "        pseudocount = 1e-10\n",
    "        log_matrix = np.log(abundance_matrix + pseudocount)\n",
    "        geometric_means = log_matrix.mean(axis=0)\n",
    "        normalized_matrix = log_matrix - geometric_means\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown normalization method: {method}\")\n",
    "    \n",
    "    # Replace the sample columns with normalized values\n",
    "    df_norm[sample_cols] = normalized_matrix\n",
    "    \n",
    "    return df_norm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "16432a7e-4bf4-4409-a836-cc0a29d186a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_genomic_data(genomic_data_path:str, rows_to_skip:int=7) -> pd.DataFrame:\n",
    "\n",
    "    # read df\n",
    "    gen_df = pd.read_csv(genomic_data_path, sep='\\t')\n",
    "    # rename gene id row to ID\n",
    "    gen_df.rename(columns={'Unnamed: 0':'ID'}, inplace=True)\n",
    "    # remove metaparams\n",
    "    gen_df = gen_df.iloc[rows_to_skip:]\n",
    "    # convert NAN to 0\n",
    "    gen_df.fillna(0, inplace=True)\n",
    "    # change values to numeric (expect geneID)\n",
    "    sample_cols = gen_df.columns.drop('ID')\n",
    "    gen_df[sample_cols] = gen_df[sample_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Remove samples with no genes\n",
    "    # 1. Calculate column sums for sample columns\n",
    "    col_sums = gen_df[sample_cols].sum()\n",
    "    # 2. Find columns with zero sum\n",
    "    zero_sum_cols = col_sums[col_sums == 0].index.tolist()\n",
    "    # 3. Remove zero-sum columns\n",
    "    if zero_sum_cols:\n",
    "        gen_df = gen_df.drop(columns=zero_sum_cols)\n",
    "\n",
    "    # reset index\n",
    "    gen_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # normalize the results per sample\n",
    "    gen_df_normalized = normalize_gene_abundances(gen_df, method='tss', id_col='ID')\n",
    "\n",
    "    # return normalized df\n",
    "    return gen_df_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b755b0a-6bca-4db8-950d-78619236796a",
   "metadata": {},
   "source": [
    "## Running genomic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0b725ddc-2185-4ec3-8ce4-5077d300c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "ben_gen_df = read_genomic_data(genomic_data_path='ifbdata/atlanteco_hack/MetaGenomics/BenguelaCurrent_GeneAb/BenguelaCurrent_ffn_GeneAb_T.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c697af46-ab65-42c0-b525-835f9d546670",
   "metadata": {},
   "outputs": [],
   "source": [
    "wedd_gen_df = read_genomic_data(genomic_data_path='ifbdata/atlanteco_hack/MetaGenomics/WeddellSea_GeneAb/WeddellSea_ffn_GeneAb_T.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd9da4-d6f2-4be7-b290-7d82294068bf",
   "metadata": {},
   "source": [
    "# Read environmental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972dcbea-6018-41ee-92fe-a47b2ee486c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1d50aa9-0694-4414-86ad-28e312342858",
   "metadata": {},
   "source": [
    "# Read Proteomics data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56059c00-3993-4d73-8267-96f76c5f93fe",
   "metadata": {},
   "source": [
    "# Read Metabolomic data (labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e222e75c-8192-41de-9e48-f49437a3a62b",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7475e099-e649-46d3-9267-5d269d0edd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_relevant_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Keep only columns starting with 'featureId' or 'SAMEA'.\n",
    "    \"\"\"\n",
    "    return df.loc[:, df.columns.str.startswith(('featureId', 'SAMEA'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1cc80d65-1f8f-433c-82e5-52ca87602535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_samea_column_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove the trailing '_RX' from SAMEA column names.\n",
    "    E.g., SAMEA123456_R01_R2 -> SAMEA123456_R01\n",
    "    \"\"\"\n",
    "    rename_map = {\n",
    "        col: re.sub(r'(SAMEA\\d+_R\\d+)_R\\d+$', r'\\1', col)\n",
    "        if col.startswith('SAMEA') else col\n",
    "        for col in df.columns\n",
    "    }\n",
    "    return df.rename(columns=rename_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "19dece8a-8641-454c-8a43-5f89fed618ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_samea_columns(df: pd.DataFrame, threshold: float = 2e4) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Group columns with the same SAMEAXXXXXX prefix:\n",
    "      - If any column in the group > threshold → grouped value = 1\n",
    "      - If all columns in the group ≤ threshold → grouped value = 0\n",
    "\n",
    "    The grouped columns will replace the original SAMEA columns.\n",
    "    \"\"\"\n",
    "    # Map each SAMEA column to its base prefix (SAMEAXXXXXX)\n",
    "    prefix_map = {\n",
    "        col: re.match(r'(SAMEA\\d+)', col).group(1)\n",
    "        if col.startswith('SAMEA') else col\n",
    "        for col in df.columns\n",
    "    }\n",
    "\n",
    "    result = df.copy()\n",
    "    for prefix in set(prefix_map.values()):\n",
    "        if prefix.startswith('SAMEA'):\n",
    "            same_cols = [col for col, pfx in prefix_map.items() if pfx == prefix]\n",
    "\n",
    "            # 🔹 Ensure these columns are numeric (convert strings → numbers, non-numeric → NaN)\n",
    "            numeric_block = df[same_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "            # ✅ Compute on numeric_block, not df\n",
    "            result[prefix] = (numeric_block > threshold).any(axis=1).astype(int)\n",
    "\n",
    "            # Drop original group columns\n",
    "            result = result.drop(columns=same_cols)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f0511265-6b33-483a-832f-da91bd8df6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_metabolomic_data(metabolome_data_path:str, threshold: float = 2e4) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "      1. Filter columns\n",
    "      2. Clean SAMEA column names\n",
    "      3. Group SAMEA columns using threshold logic\n",
    "    \"\"\"\n",
    "    metabolome_df = pd.read_csv(metabolome_data_path, sep='\\t')\n",
    "    metabolome_df.drop(metabolome_df.tail(1).index,inplace=True) # drop last row\n",
    "    metabolome_df = filter_relevant_columns(metabolome_df)\n",
    "    metabolome_df = clean_samea_column_names(metabolome_df)\n",
    "    metabolome_df = group_samea_columns(metabolome_df, threshold=threshold)\n",
    "    return metabolome_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5f5c02-77d5-42b0-a0a6-264f7ee0eed2",
   "metadata": {},
   "source": [
    "## Running metabolomic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2d74ca0a-38e9-4c71-9cd4-d3510e4966a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metabolome_path = 'ifbdata/atlanteco_hack/MetaMetabolomics/1_Feature_table_univariate_analysis_hackathon'\n",
    "\n",
    "processed_metabolomic_data = process_metabolomic_data(metabolome_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e770466a-4c7e-46ca-b11a-7e4db05aab65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    651561\n",
      "0     44715\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "processed_metabolomic_data.head()\n",
    "sample_cols = [col for col in processed_metabolomic_data.columns if col != 'featureId']\n",
    "counts = processed_metabolomic_data[sample_cols].stack().value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4d061fdb-447e-4895-9c32-7f7bdb0a1938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1842"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_metabolomic_data['featureId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b5de33-3050-45df-affa-6d6ec6b27382",
   "metadata": {},
   "source": [
    "# Next section (template header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0752ce-91a1-43bb-9194-35beb1f1c289",
   "metadata": {},
   "source": [
    "## Data loading & preprocessing helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fc104a78-5f80-4f97-9abf-bd051b32d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_metabolite_vector(val):\n",
    "    \"\"\"Parse metabolite vector strings into numpy arrays of ints.\"\"\"\n",
    "    if isinstance(val, (list, np.ndarray)):\n",
    "        return np.array(val, dtype=int)\n",
    "    s = str(val).strip()\n",
    "    try:\n",
    "        parsed = ast.literal_eval(s)\n",
    "        if isinstance(parsed, (list, tuple, np.ndarray)):\n",
    "            return np.array(parsed, dtype=int)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if ',' in s:\n",
    "        parts = s.split(',')\n",
    "    else:\n",
    "        parts = s.split()\n",
    "    return np.array([int(float(x)) for x in parts], dtype=int)\n",
    "\n",
    "def load_multiomics_dataframe(path, sep='|', sampleid_col='SampleID', metabolic_col='Metabolites_vector'):\n",
    "    \"\"\"Return feature DataFrame (X) and label DataFrame (Y).\"\"\"\n",
    "    df = pd.read_csv(path, sep=sep, engine='python')\n",
    "    if sampleid_col in df.columns:\n",
    "        df = df.set_index(sampleid_col)\n",
    "    Y_series = df[metabolic_col].apply(parse_metabolite_vector)\n",
    "    M = len(Y_series.iloc[0])\n",
    "    Y = np.vstack(Y_series.values)\n",
    "    Y_df = pd.DataFrame(Y, index=df.index, columns=[f'met_{i}' for i in range(M)])\n",
    "    X_df = df.drop(columns=[metabolic_col])\n",
    "    return X_df, Y_df\n",
    "\n",
    "def filter_labels_by_prevalence(Y, min_presence=5):\n",
    "    \"\"\"Drop labels with fewer presences than min_presence (speed + noise reduction).\"\"\"\n",
    "    keep = Y.columns[Y.sum() >= min_presence]\n",
    "    return Y[keep]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780b17db-cca9-4235-be98-3c646e5f2093",
   "metadata": {},
   "source": [
    "## Split, train, evaluate, feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5ba1bb44-2827-4678-a1a2-9ce0d4da3a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilabel_train_test_split(X, Y, test_size=0.2, random_state=42):\n",
    "    \"\"\"Multilabel-aware split: iterative if available, else random.\"\"\"\n",
    "    if ITERATIVE_AVAILABLE:\n",
    "        X_np, Y_np = X.values, Y.values\n",
    "        X_train, X_test, y_train, y_test = iterative_train_test_split(X_np, Y_np, test_size=test_size)\n",
    "        X_train, X_test = pd.DataFrame(X_train, columns=X.columns), pd.DataFrame(X_test, columns=X.columns)\n",
    "        Y_train, Y_test = pd.DataFrame(y_train, columns=Y.columns), pd.DataFrame(y_test, columns=Y.columns)\n",
    "        return X_train, X_test, Y_train, Y_test\n",
    "    else:\n",
    "        return train_test_split(X, Y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "def train_lightgbm_multioutput(X_train, Y_train):\n",
    "    \"\"\"Train LightGBM in MultiOutputClassifier wrapper.\"\"\"\n",
    "    base = LGBMClassifier(\n",
    "        objective='binary',\n",
    "        boosting_type='gbdt',\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=1,  # handled at wrapper level\n",
    "        verbosity=-1\n",
    "    )\n",
    "    model = MultiOutputClassifier(base, n_jobs=-1)\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model\n",
    "\n",
    "def evaluate_multilabel(Y_true, Y_pred, Y_score=None):\n",
    "    \"\"\"Return dict of common multilabel metrics.\"\"\"\n",
    "    out = {\n",
    "        'hamming_loss': hamming_loss(Y_true, Y_pred),\n",
    "        'micro_f1': f1_score(Y_true, Y_pred, average='micro', zero_division=0),\n",
    "        'macro_f1': f1_score(Y_true, Y_pred, average='macro', zero_division=0)\n",
    "    }\n",
    "    if Y_score is not None:\n",
    "        aucs = []\n",
    "        for i, col in enumerate(Y_true.columns):\n",
    "            yt = Y_true.iloc[:, i]\n",
    "            ys = Y_score[:, i]\n",
    "            if len(np.unique(yt)) < 2:\n",
    "                aucs.append(np.nan)\n",
    "            else:\n",
    "                aucs.append(roc_auc_score(yt, ys))\n",
    "        out['roc_auc_macro_mean'] = np.nanmean(aucs)\n",
    "    return out\n",
    "\n",
    "def extract_feature_importances(model, feature_names):\n",
    "    \"\"\"Return DataFrame: feature importance per label.\"\"\"\n",
    "    ests = model.estimators_\n",
    "    data = {}\n",
    "    for i, est in enumerate(ests):\n",
    "        data[Y_train.columns[i]] = est.feature_importances_\n",
    "    return pd.DataFrame(data, index=feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d311e0ed-a8ec-4e3e-a923-b36941c4cd00",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6584244-b832-4c43-89f6-49d89128c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Adjust to your actual path ----\n",
    "data_path = \"data_demo.csv\"   # e.g., \"my_dataset.csv\"\n",
    "\n",
    "# Load\n",
    "X, Y = load_multiomics_dataframe(data_path, sep='|')\n",
    "\n",
    "# Filter rare metabolites (optional, speeds up training)\n",
    "Y = filter_labels_by_prevalence(Y, min_presence=10)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, Y_train, Y_test = multilabel_train_test_split(X, Y)\n",
    "\n",
    "# Train\n",
    "model = train_lightgbm_multioutput(X_train, Y_train)\n",
    "\n",
    "# Predict\n",
    "Y_score = np.vstack([p[:, 1] for p in model.predict_proba(X_test)]).T\n",
    "Y_pred = (Y_score >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "metrics = evaluate_multilabel(Y_test, pd.DataFrame(Y_pred, columns=Y_test.columns), Y_score)\n",
    "print(\"Metrics:\", metrics)\n",
    "\n",
    "# Feature importances\n",
    "feat_imp = extract_feature_importances(model, X.columns)\n",
    "# Show top features for first metabolite\n",
    "print(feat_imp.iloc[:, 0].sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a203c1d-1227-41c9-8f42-e3304548860b",
   "metadata": {},
   "source": [
    "## SHAP for top metabolites (if time allows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd23c6-4614-4de7-8ddd-d412a40c8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "# Pick a label index, e.g., first metabolite\n",
    "label_idx = 0\n",
    "explainer = shap.TreeExplainer(model.estimators_[label_idx])\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test, feature_names=X.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
